{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac138205",
   "metadata": {},
   "source": [
    "### Paths to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe3bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data files and output file on drive\n",
    "train_data_file = 'train_data.json'\n",
    "val_data_file = 'valid_data.json' #needs to be modified for test file\n",
    "pred_out_file = 'prediction_out.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ecba2",
   "metadata": {},
   "source": [
    "### Importing all dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f9826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Import all dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import re\n",
    "# from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a3626",
   "metadata": {},
   "source": [
    "### Function with model to generate the conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5552256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion_generate(argument):\n",
    "    \n",
    "    inputs = tokenizer.encode(\"summarize: \" + argument, return_tensors = \"tf\", max_length = 512, truncation = True) #tokeninzing the argument\n",
    "    outputs = model.generate(inputs, max_length=150, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True) #generating the summary of the argument\n",
    "    gen_conclusion = str(tokenizer.decode(outputs[0]))\n",
    "    gen_conclusion = gen_conclusion.replace(\"<pad>\",\"\").lstrip().rstrip() #removes <pad> string generated when using t5 model\n",
    "    \n",
    "    return gen_conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dbe69",
   "metadata": {},
   "source": [
    "### Reading and preprocessing the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60c49551",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_init = pd.read_json(train_data_file)\n",
    "df_val_init = pd.read_json(val_data_file)\n",
    "df_val_init['argument'] = df_val_init['argument'].apply(lambda x: re.sub('This is a footnote.*$', '',x,flags = re.DOTALL).strip()) #removing any footnote occuring in the data\n",
    "df_val_init['argument'] = df_val_init['argument'].apply(lambda x: re.sub('gt','',x,flags = re.DOTALL).strip()) #removing gt tags observed in data\n",
    "val_args_list = df_val_init['argument'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66155dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call to conclusion generation function\n",
    "\n",
    "conclusion_list = [conclusion_generate(item) for item in tqdm(val_args_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8787e3",
   "metadata": {},
   "source": [
    "### Generating the predictions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_id_list = df_val_init['id'].to_list()\n",
    "pred_val = dict(zip(val_data_id_list, conclusion_list))\n",
    "\n",
    "with open(pred_out_file, 'w') as fp:\n",
    "    json.dump(pred_val,fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
